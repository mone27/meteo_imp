{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: gpfa.html\n",
    "title: Math\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01888fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| default_exp gpfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdc8f7",
   "metadata": {},
   "source": [
    "Derivation of the equations to solve the Gaussian Processes Factor Analysis as described in: Yu, B.M., Cunningham, J.P., Santhanam, G., Ryu, S., Shenoy, K.V., Sahani, M., 2008. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in: Advances in Neural Information Processing Systems. Curran Associates, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0f969",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- $T$  Number of time steps\n",
    "- $N$  Number of variables observed\n",
    "- $K$  Number of dimensions of latent variable\n",
    "- $x_{:,t}$ vector of all the $N$ variables at time $t$, $\\in \\mathbb{R}^N$\n",
    "- $x_{n,:}$ vector of the $n$th variable at for time steps in $T$, $\\in \\mathbb{R}^T$\n",
    "- $x_{n,t}$ $n$th variable at time $t$, $\\in \\mathbb{R}$\n",
    "- $X_M = [x_{:,1}, ... x_{:, T}]$ Matrix with all the $N$ variables at all time steps $T$, $\\in \\mathbb{R}^{N \\times T}$\n",
    "- $X$ is a vector obtained by \"flattening\" $X_M$, by putting next to each other all variable at time $t$, $\\in \\mathbb{R}^{(N \\cdot T)}$\n",
    "- $t$ time step\n",
    "- $z_{i, t}$ $i$th latent variable at time $t$, $\\in \\mathbb{R}$\n",
    "- $Z = [z_1 , ... z_t]$ Vector with $z$ at all time steps in $T$, $\\in \\mathbb{R}^{K \\times T}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc81df",
   "metadata": {},
   "source": [
    "## Gaussian Processes Factor Analysis model\n",
    "\n",
    "We model the variables in this way\n",
    " $$x_{:,t} = \\Lambda z_{:,t} + \\epsilon $$\n",
    "where:\n",
    "\n",
    "- $\\Lambda$ is a Factor loading matrix that transforms $z_{:,t}$ into $x_{:,t}$, $\\in \\mathbb{R}^{N \\times K}$\n",
    "- $\\epsilon$ Random noise. The random noise is independent between the different time steps, $\\in \\mathbb{R}^N$:\n",
    "    - $p(\\epsilon) = \\mathcal{N}(0, \\psi)$ distribution of noise\n",
    "    - $\\psi$, covariance matrix of noise, it is a diagional matrix, $\\in \\mathbb{R}^{N \\times N}$\n",
    "\n",
    "The model consider $\\langle X \\rangle = 0$ (if $X$ doesn't have a 0 mean it can be easily transformed by substracting the mean)\n",
    "\n",
    "The latent variable $z$ is modelled over time using a Gaussian Process, one process for each dimension $k$\n",
    "for simplicity we assumed that $z$ has only one dimension ($k = 1$)\n",
    "\n",
    "$$p(Z) = \\mathcal{GP}(0, k(t, t \\prime))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e65ea",
   "metadata": {},
   "source": [
    "## Derivation of $p(X)$\n",
    "\n",
    "$p(x_{:,t}|z_{:,t}) = \\mathcal{N}(\\Lambda z_{:,t}, \\psi)$ is easy to derive and then $p(x_{:,t})$ and $p(z_{:,t}|x_{:,t})$ can be obtained using the rules of Gaussian inference.\n",
    "\n",
    "However, what is interesting is to have the analytical form of $p(X)$, which models both the relations between $z$ and $x$ and the $z$ and $t$. The likelihood of $p(X)$ can then be maximized to obtain the parameters of the latent transformation and the kernel hyperparameter.\n",
    "\n",
    "$p(X)$ is a Guassian distribution with $T\\cdot N$ dimensions.\n",
    "\n",
    "$p(X) = \\mathcal{N}(\\langle X \\rangle, \\langle X X^T \\rangle)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7966cb",
   "metadata": {},
   "source": [
    "### Diagonal of the covariance matrix\n",
    "\n",
    "Let's start with the diagonal of the covariance matrix ($t = t \\prime$)\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t} + \\epsilon_{t})^T \\rangle$\n",
    "\n",
    "by multipling the two vectors together we obtain\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t}^T + \\epsilon_t \\Lambda^T z_{:,t}^T  + \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "The using the linearity of the [expectation](https://www.statlect.com/fundamentals-of-probability/expected-value-properties) we can:\n",
    "\n",
    "1) transform the expecations of a sum into a sum of expecations\n",
    "2) move the $\\Lambda$ out of the expecation, as it doesn't depend on $z$\n",
    "3) $\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle$ because $z_{:,t}$ and $\\epsilon_t$ are independent random variables\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T  \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then considering that $\\langle z_{:,t} \\rangle = 0$ and that $\\langle \\epsilon_t \\rangle = 0$\n",
    "the expression can be simplified as:\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then substituting:\n",
    "\n",
    "1) $\\langle z_{:,t} z_{:,t}^T\\rangle = k(t, t)$ as that is the covariance matrix of the Gaussian process\n",
    "2) $\\langle \\epsilon_t \\epsilon_t^T \\rangle= \\psi$\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda k(t,t)  \\Lambda^T + \\psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf159d40",
   "metadata": {},
   "source": [
    "### Off-diagonal\n",
    "\n",
    "similar to the steps of above\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t \\prime} + \\epsilon_{t \\prime})^T \\rangle$\n",
    "\n",
    "by multipling the two vectors together we obtain\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t \\prime}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t \\prime}^T + \\epsilon_t \\Lambda^T z_{:,t \\prime}^T  + \\epsilon_t \\epsilon_{t \\prime}^T \\rangle$\n",
    "\n",
    "Then using the linearity of the [expectation](https://www.statlect.com/fundamentals-of-probability/expected-value-properties) we can:\n",
    "\n",
    "1) transform the expecations of a sum into a sum of expecations\n",
    "2) move the $\\Lambda$ out of the expecatios, as it doesn't depend on t \n",
    "3) $\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle$ because $z_{:,t}$ and $\\epsilon_t$ are independent random variables\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T  \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then considering that $\\langle z_{:,t} \\rangle = 0$ and that $\\langle \\epsilon_t \\rangle = 0$\n",
    "the expression can be simplified as:\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t \\prime}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle$\n",
    "\n",
    "Then substituting:\n",
    "1) $\\langle z_{:,t} z_{:,t \\prime}^T\\rangle = k(t,t \\prime)$ as that is the covariance matrix of the Gaussian process\n",
    "2) $\\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle= 0$ as $\\epsilon_t$ and $\\epsilon_{t \\prime}$ are independent and $\\langle \\epsilon_t \\rangle = 0$\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d99835",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "The equation for the diagonal and off-diagonal element can be summarized as:\n",
    "\n",
    "$$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi$$\n",
    "\n",
    "where $\\delta(x) = \\begin{cases}\n",
    "                1 & if\\ x=0 \\\\\n",
    "                0    & if\\ x \\ne 0 \\\\\n",
    "            \\end{cases}$ \n",
    "\n",
    "Therefore $p(X)$ can be modelled as:\n",
    "\n",
    "$$p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{cccc}\n",
    "    \\Lambda k(t_1,t_1) \\Lambda^T + \\delta(1-1)\\psi & \\Lambda k(t_{1},t_{2}) \\Lambda^T + \\delta(1-2)\\psi& \\cdots & \\Lambda k(t_1 ,t_t) \\Lambda^T + \\delta(1-t)\\psi\\\\\n",
    "    \\Lambda k(t_{2},t_{1}) \\Lambda^T+ \\delta(2-1)\\psi &  \\Lambda k(t_{2},t_{2}) \\Lambda^T + \\delta(2-2)\\psi & \\cdots & \\Lambda k(t_{2},t_{t}) \\Lambda^T + \\delta(2-t)\\psi\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\Lambda k(t_{t}, t_{1}) \\Lambda^T+ \\delta(t-1)\\psi & \\Lambda k(t_{t},t_{2}) \\Lambda^T + \\delta(t-2)\\psi& \\cdots & \\Lambda k(t_{t},t_{t}) \\Lambda^T + \\delta(t-t)\\psi\\\\\n",
    "    \\end{array} } \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a137e94",
   "metadata": {},
   "source": [
    "and this is also Gaussian Process with a \"special\" kernel. Multiplying kernel with a constant ($\\Lambda$) or adding a kernel ($\\delta$) yields another valid kernel\n",
    "\n",
    "\n",
    "If we define a new kernel as $$K(t,t \\prime) = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ p(X) = \\mathcal{GP}(0, K(t, t\\prime))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92208cae",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- The parameters of the final GP ($\\Lambda, \\psi$ and the kernel hyperparameters) can be fitted by maximizing the likelihood of $p(X)$ using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af93cc0",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563b28b",
   "metadata": {},
   "source": [
    "## Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFAKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"\n",
    "    Kernel to implement Gaussian Processes Factor Analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features: int, latent_kernel: gpytorch.kernels.Kernel, latent_dims = 1, Lambda: torch.tensor = None, psi: torch.tensor = None, **kwargs):\n",
    "        \"\"\"\n",
    "        :n_features: number of variables at each time step\n",
    "        :latent_kernel: any valid GPyTorch Kernel used to model the relationship over time of the latent\n",
    "        :latent_dims: Number of latent dims, for now only 1 supported\n",
    "        :Lambda: (n_features * latent_dims) initial value for factor loading matrix\n",
    "        :psi: (n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n",
    "        \"\"\"\n",
    "        super(GPFAKernel, self).__init__(**kwargs)\n",
    "        \n",
    "        # Number of features in the X for each time step\n",
    "        self.n_features = n_features\n",
    "        assert latent_dims == 1 # Not implemented yet\n",
    "        self.latent_dims = latent_dims\n",
    "        \n",
    "        # see GPyTorch Kernels\n",
    "        self.register_parameter(\n",
    "            name = \"Lambda\",\n",
    "            parameter = torch.nn.Parameter(torch.ones(self.n_features, self.latent_dims)))\n",
    "        \n",
    "        self.latent_kernel = latent_kernel\n",
    "        \n",
    "        self.register_parameter(\n",
    "            name = \"raw_psi_diag\",\n",
    "            parameter = torch.nn.Parameter(torch.zeros(self.n_features))) \n",
    "        self.register_constraint(\"raw_psi_diag\", gpytorch.constraints.Positive())\n",
    "        if psi is not None: self.psi = psi\n",
    "    \n",
    "    # Convenient getter and setter for psi, since there is the Positive() constraint\n",
    "    @property\n",
    "    def psi(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        return self.raw_psi_diag_constraint.transform(self.raw_psi_diag)\n",
    "\n",
    "    @psi.setter\n",
    "    def psi(self, value):\n",
    "        return self._set_psi(value)\n",
    "\n",
    "    def _set_psi(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_psi_diag)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_psi_diag=self.raw_psi_diag_constraint.inverse_transform(value))\n",
    "    \n",
    "\n",
    "        \n",
    "    # perform the actual calculation\n",
    "    def forward(self, t1, t2, diag = False, last_dim_is_batch=False, **params):\n",
    "\n",
    "        # not implemented yet\n",
    "        assert diag is False\n",
    "        assert last_dim_is_batch is False\n",
    "\n",
    "        # take the number of observations from the input\n",
    "        n_obs = t1.shape[0]\n",
    "\n",
    "        # compute the latent kernel\n",
    "        kT = self.latent_kernel(t1, t2, diag, last_dim_is_batch, **params).evaluate() # this may make the whole thing slow\n",
    "       \n",
    "        return compute_gpfa_covariance(self.Lambda, kT, self.psi, self.n_features, n_obs)\n",
    "    \n",
    "    def num_outputs_per_input(self, x1,x2):\n",
    "        return self.n_features\n",
    "\n",
    "# this is a separate function, because torch script cannot take self as a parameter\n",
    "@torch.jit.script\n",
    "def compute_gpfa_covariance(Lambda, kT, psi, n_features, n_obs):\n",
    "    # pre allocate covariance matrix\n",
    "    X_cov = torch.empty(n_features * n_obs, n_features * n_obs)\n",
    "    for i in torch.arange(n_obs):\n",
    "        for j in torch.arange(n_obs):\n",
    "            # i:i+1 is required to keep the number of dimensions\n",
    "            cov =  Lambda @ kT[i:i+1,j:j+1] @ Lambda.T\n",
    "            # only diagonals add the noise\n",
    "            if i == j: cov += torch.diag(psi)\n",
    "            # add a block of size n_features*n_features to the covariance matrix\n",
    "            X_cov[i*n_features:(i*n_features + n_features),j*n_features:(j*n_features+n_features)] = cov\n",
    "    return X_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpfa_k = GPFAKernel(n_features=2, latent_kernel=gpytorch.kernels.RBFKernel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03ef6e-f086-40fc-93d0-2d71867d641f",
   "metadata": {},
   "source": [
    "The parameters are correctly registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e350e3-893a-4717-a4cb-4a1fd718b025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lambda',\n",
       "  Parameter containing:\n",
       "  tensor([[1.],\n",
       "          [1.]], requires_grad=True)),\n",
       " ('raw_psi_diag',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0.], requires_grad=True)),\n",
       " ('latent_kernel.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0.]], requires_grad=True))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gpfa_k.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f7bbf-00bf-44cc-b992-686828f4fd67",
   "metadata": {},
   "source": [
    "Check that the Kernel is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2eae6-3ba2-4a71-812b-f70e70d99a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6931, 1.0000, 0.3532, 0.3532, 0.0156, 0.0156],\n",
       "        [1.0000, 1.6931, 0.3532, 0.3532, 0.0156, 0.0156],\n",
       "        [0.3532, 0.3532, 1.6931, 1.0000, 0.3532, 0.3532],\n",
       "        [0.3532, 0.3532, 1.0000, 1.6931, 0.3532, 0.3532],\n",
       "        [0.0156, 0.0156, 0.3532, 0.3532, 1.6931, 1.0000],\n",
       "        [0.0156, 0.0156, 0.3532, 0.3532, 1.0000, 1.6931]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpfa_k(torch.tensor((1, 2, 3))).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61a3bf-4a27-49b3-b39e-d4e836b5e000",
   "metadata": {},
   "source": [
    "## GPFA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFAZeroMean(gpytorch.means.Mean):\n",
    "    \"\"\"\n",
    "    Zero Mean function to be used in GPFA, as it takes into account the number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "    def forward(self, input):\n",
    "        shape = input.shape[0] * self.n_features\n",
    "        return torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFA(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, n_features, latent_kernel):\n",
    "        super(GPFA, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = GPFAZeroMean(n_features)\n",
    "        self.covar_module = GPFAKernel(n_features, latent_kernel)\n",
    "\n",
    "    def forward(self, x, **params):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, **params)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47b618-4465-4fe7-a021-43bb38ac1c0c",
   "metadata": {},
   "source": [
    "make some very simple test data, to check that the model is working and can learn the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149005d-40cc-49db-ad14-4d112eed9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.arange(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28a241-a7a8-426b-af31-0b3cf8717cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.hstack([torch.arange(0,3) + 2* i for i in T]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874dea7-a292-4e5c-9667-9f3ea3fec34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91c2b7-61e1-4d97-a4d4-43cab85d5d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daab629-ae73-4fbf-9ccd-52f5abbc3a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497fb5a-d735-4288-b50a-4eb4144acafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPFA(T, X, likelihood, n_features = 3, latent_kernel = gpytorch.kernels.RBFKernel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b38e71-011c-40ec-b90e-582aa8bfbe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPFA(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): GPFAZeroMean()\n",
       "  (covar_module): GPFAKernel(\n",
       "    (latent_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_psi_diag_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2bb10-3f00-4864-a3f2-532033807806",
   "metadata": {},
   "source": [
    "Getting the prior from the GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1b1b7-c718-40ae-9ce0-d83f8eda933c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: torch.Size([12]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dca19-bad4-4f87-838d-89285f4d5504",
   "metadata": {},
   "source": [
    "Fitting the parameters using gradient descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e656ff1-2ca9-40aa-87c7-a33fee3f5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10 - Loss: 2.500   lengthscale: 1.240, Lambda: 1.809   noise: 1.053\n",
      "Iter 2/10 - Loss: 2.392   lengthscale: 1.312, Lambda: 1.909   noise: 0.989\n",
      "Iter 3/10 - Loss: 2.296   lengthscale: 1.386, Lambda: 1.985   noise: 0.927\n",
      "Iter 4/10 - Loss: 2.211   lengthscale: 1.460, Lambda: 2.065   noise: 0.868\n",
      "Iter 5/10 - Loss: 2.133   lengthscale: 1.535, Lambda: 2.147   noise: 0.811\n",
      "Iter 6/10 - Loss: 2.062   lengthscale: 1.611, Lambda: 2.227   noise: 0.756\n",
      "Iter 7/10 - Loss: 1.996   lengthscale: 1.686, Lambda: 2.300   noise: 0.703\n",
      "Iter 8/10 - Loss: 1.934   lengthscale: 1.761, Lambda: 2.364   noise: 0.653\n",
      "Iter 9/10 - Loss: 1.876   lengthscale: 1.835, Lambda: 2.424   noise: 0.606\n",
      "Iter 10/10 - Loss: 1.822   lengthscale: 1.908, Lambda: 2.485   noise: 0.560\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 10\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "losses = []\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(T)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, X)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, Lambda: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.latent_kernel.lengthscale.item(),\n",
    "        model.covar_module.Lambda.mean().item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a4be3-a9b4-4270-8621-df02d3083a3d",
   "metadata": {},
   "source": [
    "The model is training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ca01b-4d26-407c-9233-a407132a125c",
   "metadata": {},
   "source": [
    "Still missing:\n",
    "\n",
    "- multiple latent dimensions\n",
    "- batches in GPyTorch\n",
    "- proper optimization (writing a for loop in python is a no go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c41b65-9eb8-4d70-b4fc-05113783e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bd937-572b-4e75-93de-5bc770d28058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
